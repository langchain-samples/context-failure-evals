"""
Validation utilities for context distraction evaluation.

Contains reusable functions for extracting, validating, and comparing
agent outputs against expected calculations.
"""

import json
import re
from typing import Dict, Any, Optional, List
from collections import Counter
from langchain_openai import ChatOpenAI
from typing import TypedDict, Annotated


def extract_calculations_json(response: str) -> Dict[str, Any]:
    """
    Extract calculations JSON from markdown response.
    
    Returns empty dict if JSON is missing or invalid (format validation handled by JSON parsing).
    """
    # Try to find JSON code block first
    json_match = re.search(r'```json\s*(\{.*?\})\s*```', response, re.DOTALL)
    if not json_match:
        # Try to find JSON starting with {"calculations" - match until balanced braces
        json_match = re.search(r'(\{"calculations".*?\})', response, re.DOTALL)
    
    if json_match:
        try:
            json_str = json_match.group(1)
            # Try to parse the JSON
            data = json.loads(json_str)
            return data.get("calculations", {})
        except json.JSONDecodeError:
            pass
    return {}


def get_value_from_calculations(
    calculations_data: Dict[str, Any],
    question_index: int,
    primary_domain: str = "renewable_energy",
    secondary_domain: str = "artificial_intelligence"
) -> Optional[Any]:
    """
    Extract a specific value from calculations JSON based on question index.
    
    Question mapping:
    1: Primary domain base fact (capacity_gw or market_size_billions)
    2: Secondary domain base fact (market_size_billions or capacity_gw)
    3: Primary domain compound_growth_10yr
    4: Primary domain CBA NPV (cba_10pct.npv)
    5: Primary domain correlation_market_size_vs_growth
    6: Primary domain market_share_top_segment_percent
    7: Primary domain investment_priority_rank
    8: Primary domain risk_adjusted_npv
    9: Primary domain weighted_investment_score
    10: Primary domain strategic_priority_rank
    """
    domain_data = calculations_data.get(primary_domain, {})
    
    if question_index == 1:
        base_facts = domain_data.get("base_facts", {})
        return base_facts.get("capacity_gw") or base_facts.get("market_size_billions")
    elif question_index == 2:
        secondary_data = calculations_data.get(secondary_domain, {})
        base_facts = secondary_data.get("base_facts", {})
        return base_facts.get("market_size_billions") or base_facts.get("capacity_gw")
    elif question_index == 3:
        return domain_data.get("compound_growth_10yr")
    elif question_index == 4:
        cba = domain_data.get("cba_10pct", {})
        return cba.get("npv") if isinstance(cba, dict) else None
    elif question_index == 5:
        return domain_data.get("correlation_market_size_vs_growth")
    elif question_index == 6:
        return domain_data.get("market_share_top_segment_percent")
    elif question_index == 7:
        return domain_data.get("investment_priority_rank")
    elif question_index == 8:
        return domain_data.get("risk_adjusted_npv")
    elif question_index == 9:
        return domain_data.get("weighted_investment_score")
    elif question_index == 10:
        return domain_data.get("strategic_priority_rank")
    
    return None


def compare_values(actual_value: Any, expected_value: str, tolerance: float = 0.01) -> bool:
    """
    Compare actual value with expected value.
    
    Args:
        actual_value: The actual value from agent output
        expected_value: The expected value as a string
        tolerance: Numeric tolerance (default 1%)
    
    Returns:
        True if values match within tolerance
    """
    if actual_value is None:
        return False
    
    try:
        expected_num = float(expected_value)
        actual_num = float(actual_value)
        # Check within tolerance
        if abs(actual_num - expected_num) / max(abs(expected_num), 1) < tolerance:
            return True
    except (ValueError, TypeError):
        # String comparison
        if str(actual_value) == str(expected_value):
            return True
    
    return False


class ConsistencyCheck(TypedDict):
    """Check if markdown and JSON are consistent."""
    is_consistent: Annotated[bool, "True if markdown and JSON values match"]
    inconsistencies: Annotated[List[str], "List of specific inconsistencies found with exact values from both sources"]
    consistency_score: Annotated[float, "Score from 0.0 to 1.0"]
    reasoning: Annotated[str, "Detailed explanation of consistency check, including methodology"]
    specific_examples: Annotated[List[Dict[str, str]], "List of specific examples showing conflicts, each with 'markdown_value', 'json_value', 'field_name', and 'description'"]


def check_consistency_with_llm(
    response: str,
    calculations_json: Dict[str, Any],
    model: str = "gpt-4o-mini"
) -> Dict[str, Any]:
    """Use LLM to check consistency between markdown and JSON."""
    judge_llm = ChatOpenAI(model=model, temperature=0)
    judge = judge_llm.with_structured_output(ConsistencyCheck)
    
    prompt = f"""You are checking consistency between markdown content and JSON data in a research report.

Markdown Content:
{response[:4000]}

JSON Data:
{json.dumps(calculations_json, indent=2)[:2000]}

Your task is to verify that values mentioned in the markdown text match the values in the JSON section.

CHECK THESE SPECIFIC FIELDS FOR ALL DOMAINS:
- Base facts: capacity_gw, market_size_billions, battery_cost_kwh, qubits, growth_rate
- Compound growth: compound_growth_10yr values
- CBA: cba_10pct.npv and cba_10pct.roi values
- Correlations: correlation_market_size_vs_growth values
- Market share: market_share_top_segment_percent values
- Rankings: investment_priority_rank, strategic_priority_rank values
- Scores: risk_adjusted_npv, weighted_investment_score values

TOLERANCE: Consider numeric values consistent if they match within 5% tolerance.

REQUIREMENTS:
1. For each inconsistency found, provide a SPECIFIC EXAMPLE in the 'specific_examples' list with:
   - field_name: The exact field name (e.g., "renewable_energy.compound_growth_10yr")
   - markdown_value: The exact value mentioned in markdown (quote the text) or "N/A" if not found
   - json_value: The exact value from JSON
   - description: Brief explanation of the conflict (e.g., "Markdown value differs from JSON")

2. In 'inconsistencies', list each conflict as a clear sentence (e.g., "Markdown states 8433.21 but JSON shows 8000.00")

3. In 'reasoning', explain:
   - Your methodology for checking consistency
   - How you searched the markdown for values
   - How you compared markdown to JSON
   - Overall assessment of consistency across all domains and fields

4. Set 'consistency_score' to 1.0 if all values match, decreasing proportionally for each inconsistency found.

Be thorough and specific - check ALL statistics, not just growth rates. A human reviewer needs to be able to verify your findings."""

    try:
        result = judge.invoke(prompt)
        return {
            "score": result.get("consistency_score", 0.0),
            "is_consistent": result.get("is_consistent", False),
            "inconsistencies": result.get("inconsistencies", []),
            "reasoning": result.get("reasoning", ""),
            "specific_examples": result.get("specific_examples", []),
        }
    except Exception as e:
        return {
            "score": 0.0,
            "is_consistent": False,
            "inconsistencies": [f"Error checking consistency: {str(e)}"],
            "reasoning": f"Failed to check consistency: {str(e)}",
            "specific_examples": [],
        }


def generate_expected_tool_calls(
    topics: List[str],
    stats_count: int,
    expert_count: int,
    case_count: int,
    year_count: int,
    compare_count: int,
) -> List[Dict[str, Any]]:
    """Generate expected tool calls pattern based on task structure."""
    expected = []
    for topic in topics:
        expected.append({"name": "research_topic", "args": {"topic": topic}})
        for i in range(stats_count):
            expected.append({"name": "get_statistics", "args": {"topic": topic}})
        for i in range(expert_count):
            expected.append({"name": "get_expert_opinion", "args": {"topic": topic}})
        for i in range(case_count):
            expected.append({"name": "get_case_study", "args": {"topic": topic}})
        for i in range(year_count):
            expected.append({"name": "get_year_data", "args": {"topic": topic}})
        expected.append({"name": "calculate_compound_growth", "args": {"topic": topic}})
        expected.append({"name": "calculate_market_share", "args": {"topic": topic}})
        expected.append({"name": "analyze_correlation", "args": {"topic": topic}})
        expected.append({"name": "calculate_cost_benefit_analysis", "args": {"topic": topic}})
        expected.append({"name": "aggregate_statistics", "args": {"topic": topic}})
        for i in range(compare_count):
            expected.append({"name": "compare_topics", "args": {"topic": topic}})
        expected.append({"name": "synthesize_research", "args": {"topics": topics}})
    return expected


def _normalize_tool_call(tc: Dict[str, Any]) -> Dict[str, Any]:
    """Normalize a tool call for comparison (handle different argument formats)."""
    normalized = {"name": tc.get("name", "")}
    args = tc.get("args", {})
    if isinstance(args, dict):
        normalized["args"] = {k: v for k, v in args.items() if v is not None}
    else:
        normalized["args"] = {}
    return normalized


def _tool_call_matches(actual: Dict[str, Any], expected: Dict[str, Any]) -> bool:
    """Check if actual tool call matches expected (name + arguments)."""
    if actual.get("name") != expected.get("name"):
        return False
    
    actual_args = actual.get("args", {})
    expected_args = expected.get("args", {})
    
    # Check that all expected arguments are present and match exactly
    for key, expected_value in expected_args.items():
        if key not in actual_args:
            return False
        actual_value = actual_args[key]
        
        # Exact match required (tolerance = 0)
        if isinstance(expected_value, (int, float)) and isinstance(actual_value, (int, float)):
            if expected_value != actual_value:
                return False
        elif expected_value != actual_value:
            return False
    
    return True


def compare_tool_calls(
    actual_tool_calls: List[Dict[str, Any]],
    expected_tool_calls: List[Dict[str, Any]],
    strict_order: bool = False
) -> Dict[str, Any]:
    """
    Compare actual tool calls with expected tool calls.
    
    Checks that expected tool calls with matching arguments are PRESENT in the actual tool calls,
    regardless of order (unless strict_order=True).
    
    Args:
        actual_tool_calls: List of actual tool calls with {"name": str, "args": dict}
        expected_tool_calls: List of expected tool calls with {"name": str, "args": dict}
        strict_order: If True, also check order. If False, only check presence.
    
    Returns:
        Dict with score, matched_steps, unmatched_steps, total_expected, and unmatched_indices
    """
    if not expected_tool_calls:
        return {
            "score": 0.0,
            "matched_steps": 0,
            "unmatched_steps": len(actual_tool_calls),
            "total_expected": 0,
            "unmatched_indices": [],
        }
    
    # Normalize both tool call lists
    actual_normalized = [_normalize_tool_call(tc) for tc in actual_tool_calls]
    expected_normalized = [_normalize_tool_call(tc) for tc in expected_tool_calls]
    
    if strict_order:
        # Check order: match at each position
        matches = 0
        unmatched_indices = []
        for idx, expected_tc in enumerate(expected_normalized):
            if idx < len(actual_normalized):
                if _tool_call_matches(actual_normalized[idx], expected_tc):
                    matches += 1
                else:
                    unmatched_indices.append(idx)
            else:
                unmatched_indices.append(idx)
        
        matched_steps = matches
        unmatched_steps = len(expected_normalized) - matches
    else:
        # Check presence: find each expected tool call somewhere in actual tool calls
        matched_indices = []
        unmatched_indices = []
        
        for idx, expected_tc in enumerate(expected_normalized):
            found = False
            for actual_tc in actual_normalized:
                if _tool_call_matches(actual_tc, expected_tc):
                    matched_indices.append(idx)
                    found = True
                    break
            if not found:
                unmatched_indices.append(idx)
        
        matched_steps = len(matched_indices)
        unmatched_steps = len(unmatched_indices)
    
    total_expected = len(expected_normalized)
    score = matched_steps / total_expected if total_expected > 0 else 0.0
    
    return {
        "score": min(score, 1.0),
        "matched_steps": matched_steps,
        "unmatched_steps": unmatched_steps,
        "total_expected": total_expected,
        "unmatched_indices": unmatched_indices,
    }

