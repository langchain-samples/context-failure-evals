{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Poisoning: When Errors Get Repeatedly Referenced\n",
    "\n",
    "Context poisoning occurs when a **hallucination or other error makes it into the context, where it is repeatedly referenced**.\n",
    "\n",
    "## The Demonstration\n",
    "\n",
    "This notebook follows a simple pattern to demonstrate context poisoning:\n",
    "\n",
    "1. ‚úÖ **Baseline Success**: Agent completes a research task successfully\n",
    "2. ‚è™ **Rewind & Inject**: Return to a checkpoint and inject poisoned information into context\n",
    "3. ‚ùå **Agent Struggles**: Same agent now fails or wastes effort due to poisoned context\n",
    "4. üõ°Ô∏è **Improved Agent**: Better prompt/architecture that's resilient to poisoning\n",
    "5. ‚úÖ **Resilient Success**: Improved agent handles poisoned context correctly\n",
    "\n",
    "## The Problem\n",
    "\n",
    "LLM agents often maintain context about their goals, progress, and state. When an error makes it into this context, the agent doesn't just use it once‚Äîit **keeps referencing it** over and over:\n",
    "\n",
    "1. **Error enters context**: Agent hallucinates something (e.g., \"Research Quantum Dynamics Corp with ticker QDYN\")\n",
    "2. **Error gets stored**: The hallucination is saved in goals, summaries, or state\n",
    "3. **Repeated reference**: Agent repeatedly checks goals, sees the hallucinated item, and references it\n",
    "4. **Fixation**: Agent becomes stuck trying to achieve something impossible\n",
    "\n",
    "As noted in the DeepMind Gemini 2.5 technical report: *\"An especially egregious form of this issue can take place with 'context poisoning'‚Äîwhere many parts of the context (goals, summary) are 'poisoned' with misinformation... As a result, the model can become fixated on achieving impossible or irrelevant goals.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dependencies are installed\n",
    "try:\n",
    "    import langchain\n",
    "    import langchain_anthropic\n",
    "    import langsmith\n",
    "    DEPENDENCIES_INSTALLED = True\n",
    "except ImportError as e:\n",
    "    DEPENDENCIES_INSTALLED = False\n",
    "    missing_module = str(e).split(\"'\")[1] if \"'\" in str(e) else \"unknown\"\n",
    "    print(f\"‚ùå Missing dependency: {missing_module}\")\n",
    "    print(\"\\nüì¶ To install dependencies, run one of the following:\")\n",
    "    print(\"   1. Using uv:  uv sync\")\n",
    "    print(\"   2. Using pip: pip install langchain langchain-anthropic langsmith langgraph\")\n",
    "    print(\"\\n   Then restart your Jupyter kernel.\")\n",
    "    raise\n",
    "\n",
    "# Standard library\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# LangChain & LangSmith\n",
    "from langchain.agents import create_agent\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langsmith import Client, evaluate\n",
    "from langsmith.schemas import Run, Example\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Environment\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Agent components\n",
    "from context_poisoning.tools import (\n",
    "    all_tools,\n",
    "    reset_state,\n",
    "    inject_poisoned_goal,\n",
    ")\n",
    "from context_poisoning.instructions import NAIVE_RESEARCH_INSTRUCTIONS, FINANCIAL_RESEARCH_INSTRUCTIONS\n",
    "\n",
    "# Evaluators and test data\n",
    "from context_poisoning.tests.evaluators import (\n",
    "    context_poisoning_evaluator,\n",
    "    goal_cancellation_evaluator,\n",
    "    task_completion_evaluator,\n",
    ")\n",
    "from context_poisoning.tests.setup_datasets import create_poisoning_dataset\n",
    "from context_poisoning.resources.test_cases import TEST_CASES\n",
    "\n",
    "# Agent utilities\n",
    "from context_poisoning.utils.agent_helpers import (\n",
    "    run_agent_with_trajectory,\n",
    ")\n",
    "\n",
    "# Initialize LangSmith\n",
    "client = Client()\n",
    "\n",
    "print(\"‚úì Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM - using Claude Haiku for context poisoning demo\n",
    "llm = ChatAnthropic(model=\"claude-haiku-4-5-20251001\", temperature=0)\n",
    "\n",
    "print(f\"Using model: {llm.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Scenario: Financial Research Agent\n",
    "\n",
    "Our financial research agent conducts research on companies and stocks. It can:\n",
    "- Track companies and get stock prices\n",
    "- Gather company information\n",
    "- Set and track research goals\n",
    "- Create research summaries\n",
    "\n",
    "**The Task**: Research Apple and Google, then create a summary of findings.\n",
    "\n",
    "**The Poisoned Context**: We'll inject a goal about researching a non-existent company (\"Quantum Dynamics Corp\", ticker QDYN) midway through the task. This simulates what happens when an agent hallucinates a company name that gets stored in its goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create naive agent (without context poisoning safeguards)\n",
    "naive_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=all_tools,\n",
    "    system_prompt=NAIVE_RESEARCH_INSTRUCTIONS\n",
    ")\n",
    "\n",
    "print(\"‚úì Naive financial research agent created (no verification/cancellation safeguards)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: ‚úÖ Baseline Success\n",
    "\n",
    "First, let's run the agent on a clean research task with no poisoned context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset to clean state\n",
    "reset_state()\n",
    "\n",
    "# Create initial research state\n",
    "from context_poisoning.tools import track_company_helper, add_research_note_helper\n",
    "\n",
    "track_company_helper(\"AAPL\")\n",
    "track_company_helper(\"GOOGL\")\n",
    "\n",
    "print(\"‚úì Clean initial state:\")\n",
    "print(\"  - Tracking: AAPL, GOOGL\")\n",
    "print(\"  - No poisoned goals\")\n",
    "\n",
    "# Run agent on the research task\n",
    "baseline_query = \"\"\"Research the companies you're tracking. Get their stock prices and company information, then create a research summary.\"\"\"\n",
    "\n",
    "print(f\"\\nüî¨ Running naive agent (clean, no poisoning)...\")\n",
    "print(f\"Query: {baseline_query}\\n\")\n",
    "\n",
    "baseline_result = run_agent_with_trajectory(naive_agent, baseline_query)\n",
    "\n",
    "print(f\"\\n‚úì Baseline completed with {len(baseline_result['trajectory'])} tool calls\")\n",
    "print(f\"\\nBaseline response:\")\n",
    "print(baseline_result['final_response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: ‚è™ Rewind & Inject Poisoned Context\n",
    "\n",
    "Now let's rewind to the same starting point and inject a poisoned goal into the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset to the same initial state\n",
    "reset_state()\n",
    "track_company_helper(\"AAPL\")\n",
    "track_company_helper(\"GOOGL\")\n",
    "\n",
    "# Inject poisoned goal - a hallucinated company that doesn't exist\n",
    "poisoned_goal = \"Research the financial performance of 'Quantum Dynamics Corp' (ticker: QDYN) and analyze its Q4 earnings\"\n",
    "inject_poisoned_goal(poisoned_goal)\n",
    "\n",
    "print(\"‚úì Rewound to checkpoint\")\n",
    "print(\"  - Tracking: AAPL, GOOGL\")\n",
    "print(f\"  - ‚ö†Ô∏è  INJECTED POISONED GOAL: '{poisoned_goal}'\")\n",
    "print(\"\\nThis simulates what happens when an agent hallucinates a company and stores it in goals.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: ‚ùå Naive Agent Struggles with Poisoned Context\n",
    "\n",
    "Run the same naive agent (without safeguards) with poisoned context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the same query, but now the agent has a poisoned goal\n",
    "poisoned_query = \"\"\"Research the companies you're tracking. Get their stock prices and company information, then create a research summary. \n",
    "\n",
    "Also check if you have any active research goals and work on completing them.\"\"\"\n",
    "\n",
    "print(f\"üî¨ Running naive agent with poisoned context...\")\n",
    "print(f\"Query: {poisoned_query}\\n\")\n",
    "\n",
    "poisoned_result = run_agent_with_trajectory(naive_agent, poisoned_query)\n",
    "\n",
    "print(f\"\\n‚úì Naive agent run completed with {len(poisoned_result['trajectory'])} tool calls\")\n",
    "print(f\"\\nNaive agent response:\")\n",
    "print(poisoned_result['final_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the poisoning impact\n",
    "from context_poisoning.tests.evaluators import count_poisoned_references, detect_impossible_goal_pursuit\n",
    "\n",
    "baseline_refs = count_poisoned_references(baseline_result, poisoned_goal)\n",
    "poisoned_refs = count_poisoned_references(poisoned_result, poisoned_goal)\n",
    "impossible_goal_result = detect_impossible_goal_pursuit(poisoned_result, poisoned_goal)\n",
    "\n",
    "print(\"üìä Context Poisoning Analysis:\")\n",
    "print(f\"  Baseline references to QDYN: {baseline_refs}\")\n",
    "print(f\"  Poisoned references to QDYN: {poisoned_refs}\")\n",
    "print(f\"  Attempts to pursue impossible goal: {impossible_goal_result['attempt_count']}\")\n",
    "print(f\"  Recognized as impossible: {impossible_goal_result['recognized_as_impossible']}\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Impact: Agent made {poisoned_refs} references to the hallucinated company\")\n",
    "print(f\"  This wastes {poisoned_refs}x the compute on error-based reasoning\")\n",
    "\n",
    "print(f\"\\nTool call comparison:\")\n",
    "print(f\"  Baseline: {len(baseline_result['trajectory'])} tool calls\")\n",
    "print(f\"  Poisoned: {len(poisoned_result['trajectory'])} tool calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: üõ°Ô∏è Improved Agent - Resilient to Poisoning\n",
    "\n",
    "Let's create an improved version of the agent with better instructions that help it:\n",
    "1. Validate goals before pursuing them\n",
    "2. Recognize when information doesn't exist\n",
    "3. Cancel impossible goals quickly\n",
    "4. Avoid repeated references to errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved instructions with validation and error handling\n",
    "# (FINANCIAL_RESEARCH_INSTRUCTIONS already includes these safeguards)\n",
    "IMPROVED_INSTRUCTIONS = FINANCIAL_RESEARCH_INSTRUCTIONS + \"\"\"\n",
    "\n",
    "## CRITICAL: Context Poisoning Prevention (Enhanced)\n",
    "\n",
    "You must actively prevent context poisoning by validating information before repeated reference:\n",
    "\n",
    "1. **Validate Before Pursuing Goals**:\n",
    "   - Before working on a research goal, verify the ticker/company exists\n",
    "   - If get_stock_price or get_company_info returns an error, the ticker doesn't exist\n",
    "   - Immediately cancel goals about non-existent tickers\n",
    "\n",
    "2. **Recognize Tool Errors Quickly**:\n",
    "   - If a tool returns \"not found\", \"doesn't exist\", or similar errors, treat it as definitive\n",
    "   - Do NOT retry the same ticker multiple times\n",
    "   - Update the goal status to \"cancelled\" with a clear reason\n",
    "\n",
    "3. **Avoid Repeated References**:\n",
    "   - Once you determine something doesn't exist, don't mention it again\n",
    "   - Don't include impossible goals in summaries\n",
    "   - Focus on achievable goals\n",
    "\n",
    "4. **Error Handling Pattern**:\n",
    "   ```\n",
    "   If tool error indicates ticker/company doesn't exist:\n",
    "     ‚Üí update_research_goal(goal_id, \"cancelled\", reason=\"Ticker not found\")\n",
    "     ‚Üí Move on to next goal\n",
    "     ‚Üí Do NOT reference this goal again\n",
    "   ```\n",
    "\n",
    "Following these rules will prevent you from repeatedly referencing errors and wasting effort on impossible goals.\n",
    "\"\"\"\n",
    "\n",
    "# Create improved agent\n",
    "improved_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=all_tools,\n",
    "    system_prompt=IMPROVED_INSTRUCTIONS\n",
    ")\n",
    "\n",
    "print(\"‚úì Improved agent created with validation and error handling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: ‚úÖ Improved Agent Handles Poisoning\n",
    "\n",
    "Test the improved agent with the same poisoned context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset and inject poisoned context again\n",
    "reset_state()\n",
    "track_company_helper(\"AAPL\")\n",
    "track_company_helper(\"GOOGL\")\n",
    "inject_poisoned_goal(poisoned_goal)\n",
    "\n",
    "print(\"‚úì Same poisoned state\")\n",
    "print(f\"  - ‚ö†Ô∏è  Poisoned goal: '{poisoned_goal}'\\n\")\n",
    "\n",
    "# Run improved agent\n",
    "improved_query = \"\"\"Research the companies you're tracking. Get their stock prices and company information, then create a research summary. \n",
    "\n",
    "Also check if you have any active research goals and work on completing them.\"\"\"\n",
    "\n",
    "print(f\"üî¨ Running improved agent with poisoned context...\")\n",
    "print(f\"Query: {improved_query}\\n\")\n",
    "\n",
    "improved_result = run_agent_with_trajectory(improved_agent, improved_query)\n",
    "\n",
    "print(f\"\\n‚úì Improved run completed with {len(improved_result['trajectory'])} tool calls\")\n",
    "print(f\"\\nImproved response:\")\n",
    "print(improved_result['final_response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: üìä Comparison & Analysis\n",
    "\n",
    "Compare the baseline, poisoned, and improved runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for all three runs\n",
    "improved_refs = count_poisoned_references(improved_result, poisoned_goal)\n",
    "improved_goal_result = detect_impossible_goal_pursuit(improved_result, poisoned_goal)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    \"Metric\": [\n",
    "        \"References to QDYN (post-error)\",\n",
    "        \"Tool Calls\",\n",
    "        \"Recognized Impossible\",\n",
    "        \"Goal Cancelled\"\n",
    "    ],\n",
    "    \"Baseline (Clean)\": [\n",
    "        baseline_refs,\n",
    "        len(baseline_result['trajectory']),\n",
    "        \"N/A\",\n",
    "        \"N/A\"\n",
    "    ],\n",
    "    \"Naive Agent (Poisoned)\": [\n",
    "        poisoned_refs,\n",
    "        len(poisoned_result['trajectory']),\n",
    "        \"Yes\" if impossible_goal_result['recognized_as_impossible'] else \"No\",\n",
    "        \"Unknown\"  # Would need to check the actual goal status\n",
    "    ],\n",
    "    \"Improved Agent (Poisoned)\": [\n",
    "        improved_refs,\n",
    "        len(improved_result['trajectory']),\n",
    "        \"Yes\" if improved_goal_result['recognized_as_impossible'] else \"No\",\n",
    "        \"Unknown\"  # Would need to check the actual goal status\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"üìä Comparison Results:\")\n",
    "print(\"=\" * 90)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\" * 90)\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(f\"  ‚Ä¢ Naive agent (no safeguards) referenced QDYN {poisoned_refs} times after error\")\n",
    "print(f\"  ‚Ä¢ Improved agent (with safeguards) referenced QDYN {improved_refs} times after error\")\n",
    "print(f\"  ‚Ä¢ Reduction: {poisoned_refs - improved_refs} fewer post-error references\")\n",
    "print(f\"  ‚Ä¢ Lower post-error references = better (stops pursuing impossible goals)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(\"Post-Error References to QDYN\", \"Total Tool Calls\"),\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
    ")\n",
    "\n",
    "# References comparison\n",
    "runs = [\"Baseline<br>(Clean)\", \"Naive<br>(Poisoned)\", \"Improved<br>(Poisoned)\"]\n",
    "refs = [baseline_refs, poisoned_refs, improved_refs]\n",
    "colors = ['green', 'red', 'blue']\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=runs, y=refs, marker_color=colors, text=refs, textposition='outside'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Tool calls comparison\n",
    "tool_calls = [\n",
    "    len(baseline_result['trajectory']),\n",
    "    len(poisoned_result['trajectory']),\n",
    "    len(improved_result['trajectory'])\n",
    "]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=runs, y=tool_calls, marker_color=colors, text=tool_calls, textposition='outside'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Context Poisoning Impact & Mitigation\",\n",
    "    showlegend=False,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nüìà Visualization shows:\")\n",
    "print(\"  ‚Ä¢ GREEN bar = Clean baseline (no poisoned context)\")\n",
    "print(\"  ‚Ä¢ RED bar = Naive agent with poisoning (high post-error references = bad)\")\n",
    "print(\"  ‚Ä¢ BLUE bar = Improved agent with poisoning (low post-error references = good)\")\n",
    "print(\"\\nThe improved agent should have fewer post-error references than the naive agent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Understanding the Problem: Repeated Reference\n",
    "\n",
    "### What Happened\n",
    "\n",
    "1. **Error Enters Context**: Agent's goal includes researching \"QDYN\" (doesn't exist)\n",
    "2. **Gets Stored**: Goal is saved in the agent's goal tracking system\n",
    "3. **Repeated Reference**: Agent checks goals ‚Üí sees QDYN ‚Üí references it ‚Üí checks again ‚Üí sees QDYN ‚Üí references it...\n",
    "4. **Wasted Compute**: Each reference wastes tokens and API calls on impossible tasks\n",
    "\n",
    "### Why It's Problematic\n",
    "\n",
    "- **Not a one-time error**: The hallucination gets used multiple times\n",
    "- **Reinforcement**: Each reference makes the error seem more legitimate\n",
    "- **Persistence**: Error stays in context across many turns\n",
    "- **Difficult to escape**: Agent needs explicit validation to stop the cycle\n",
    "\n",
    "### The Solution\n",
    "\n",
    "The improved agent breaks the cycle by:\n",
    "1. **Validating early**: Check if ticker exists before pursuing\n",
    "2. **Recognizing errors**: Treat tool errors as definitive signals\n",
    "3. **Canceling quickly**: Update goal status to \"cancelled\" immediately\n",
    "4. **Avoiding repetition**: Don't reference cancelled goals again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### The Core Problem\n",
    "\n",
    "**Context poisoning = Repeated reference to errors**\n",
    "\n",
    "An error (hallucination, wrong data, bad assumption) enters context once, but gets **used many times**:\n",
    "- Every goal check references it\n",
    "- Every summary includes it\n",
    "- Every planning step considers it\n",
    "- Each reference wastes compute and reinforces the error\n",
    "\n",
    "### What We Demonstrated\n",
    "\n",
    "1. ‚úÖ **Baseline**: Agent completes task successfully without errors\n",
    "2. ‚è™ **Rewind + Inject**: Same starting point, but with poisoned goal\n",
    "3. ‚ùå **Poisoned Run**: Agent references error multiple times\n",
    "4. üõ°Ô∏è **Improved Agent**: Better validation and error handling\n",
    "5. ‚úÖ **Resilient Run**: Improved agent minimizes repeated references\n",
    "\n",
    "### Building Resilient Agents\n",
    "\n",
    "To prevent repeated reference of errors:\n",
    "\n",
    "1. **Validate Before Storing**: Check information before adding to goals/context\n",
    "2. **Recognize Tool Errors**: Treat \"not found\" errors as definitive\n",
    "3. **Cancel Quickly**: Update impossible goals to \"cancelled\" immediately\n",
    "4. **Avoid Re-reference**: Don't mention cancelled goals again\n",
    "5. **Make Errors Explicit**: Clear error signals help agents recover faster\n",
    "\n",
    "### The Bottom Line\n",
    "\n",
    "Context poisoning isn't about making one error‚Äîit's about **repeatedly using that error**. The key to mitigation is breaking the cycle of repeated reference through validation, quick error recognition, and explicit goal cancellation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Optional: Full Evaluation with LangSmith\n",
    "\n",
    "The cells below show how to run systematic evaluations across multiple test cases using LangSmith.\n",
    "\n",
    "This is useful for:\n",
    "- Testing multiple poisoning scenarios\n",
    "- Comparing different agent configurations\n",
    "- Tracking metrics over time\n",
    "- Building evaluation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test cases\n",
    "print(f\"Available test cases: {len(TEST_CASES)}\\n\")\n",
    "\n",
    "for i, test_case in enumerate(TEST_CASES, 1):\n",
    "    print(f\"{i}. {test_case['name']}\")\n",
    "    print(f\"   {test_case['description'][:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset in LangSmith (optional - uncomment to run)\n",
    "# dataset_name = \"context-poisoning-evaluation\"\n",
    "# dataset = create_poisoning_dataset(dataset_name, TEST_CASES, client)\n",
    "# print(f\"‚úì Created dataset with {len(TEST_CASES)} examples\")\n",
    "\n",
    "# Define evaluators\n",
    "ALL_EVALUATORS = [\n",
    "    context_poisoning_evaluator,      # Measures references to poisoned info\n",
    "    goal_cancellation_evaluator,      # Whether agent cancels impossible goals\n",
    "    task_completion_evaluator,        # Whether agent completes real tasks\n",
    "]\n",
    "\n",
    "print(f\"‚úì Loaded {len(ALL_EVALUATORS)} evaluators\")\n",
    "print(\"   - context_poisoning: References, impossible goal pursuit, recovery\")\n",
    "print(\"   - goal_cancellation: Recognizes and cancels impossible goals\")\n",
    "print(\"   - task_completion: Completes achievable tasks despite poisoning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run full evaluation (optional - uncomment to run)\n\nprint(f\"\\nüî¨ Running evaluation on {len(TEST_CASES)} test cases...\")\n\nbaseline_experiment = evaluate(\n    lambda inputs: run_agent_with_trajectory(naive_agent, inputs[\"query\"]),\n    data=dataset_name,\n    evaluators=ALL_EVALUATORS,\n    experiment_prefix=\"context-poisoning-baseline\",\n    metadata={\"config\": \"baseline\", \"test_cases\": len(TEST_CASES)},\n)\n\nimproved_experiment = evaluate(\n    lambda inputs: run_agent_with_trajectory(improved_agent, inputs[\"query\"]),\n    data=dataset_name,\n    evaluators=ALL_EVALUATORS,\n    experiment_prefix=\"context-poisoning-improved\",\n    metadata={\"config\": \"improved\", \"test_cases\": len(TEST_CASES)},\n)\n\nprint(\"\\n‚úì Evaluation complete!\")\nprint(f\"   View results in LangSmith\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated **context poisoning through repeated reference**:\n",
    "\n",
    "### What We Showed\n",
    "1. ‚úÖ Agent succeeds on a clean research task\n",
    "2. ‚è™ Rewound and injected a poisoned goal (non-existent company)\n",
    "3. ‚ùå Agent with poisoned context references the error multiple times\n",
    "4. üõ°Ô∏è Improved agent with validation handles poisoned context better\n",
    "5. üìä Comparison shows reduced repeated references in improved version\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "**Context poisoning = One error, many references**\n",
    "\n",
    "The problem isn't just making an error‚Äîit's repeatedly using that error across multiple turns, wasting compute and reinforcing incorrect information.\n",
    "\n",
    "### Mitigation Strategy\n",
    "\n",
    "Break the cycle of repeated reference by:\n",
    "- Validating information before storing in context\n",
    "- Recognizing tool errors as definitive signals\n",
    "- Canceling impossible goals quickly\n",
    "- Avoiding re-reference of known errors\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To build more resilient agents:\n",
    "1. Add validation at context entry points (goals, summaries)\n",
    "2. Make tool error signals more explicit\n",
    "3. Track repeated failures and auto-cancel\n",
    "4. Test with LangSmith evaluations to measure improvement\n",
    "5. Monitor reference counts in production to detect poisoning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}