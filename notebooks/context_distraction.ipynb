{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Context Distraction\n\nThis notebook compares standard ReAct agent performance against a Multi-Agent (supervisor/worker delegation pattern) on context distraction tasks.\n\n## What We're Testing\n\nBoth agents answer 8 questions (Q1-8) requiring research across 5 technology domains. The Multi-Agent uses a supervisor that delegates to workers with isolated context, while the Standard Agent processes everything in a single context.\n\n**Hypothesis**: Multi-Agent's context isolation should improve recall accuracy on multi-step tasks."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imports and Setup\nimport asyncio\nfrom typing import Dict, Any, List\nimport pandas as pd\nimport numpy as np\nfrom IPython.display import display, Markdown\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\n# Import test infrastructure\nfrom context_distraction.resources.test_tasks import TEST_TASKS, build_partial_task\nfrom context_distraction.tests.evaluators import recall_accuracy_evaluator\nfrom context_distraction.tests.setup_datasets import build_reference_outputs\nfrom context_distraction.resources.validation_utils import extract_tool_calls_from_message\n\n# Import agents\nfrom context_distraction.agent import agent as standard_agent\nfrom context_distraction.multi import run_multi_agent\n\nprint(\"Setup complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Configuration\n",
    "\n",
    "Running Q1-8 on all 3 tasks with multiple trials to measure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Configuration:\n",
      "  Questions: [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  Trials per task: 3\n",
      "\n",
      "Tasks:\n",
      "  Task 1: Task 1: 5 Domains - Focus on Renewable Energy (focus: renewable_energy)\n",
      "  Task 2: Task 2: 5 Domains - Focus on Electric Vehicles (focus: electric_vehicles)\n",
      "  Task 3: Task 3: 5 Domains - Focus on Biotechnology (focus: biotechnology)\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "QUESTIONS = [1, 2, 3, 4, 5, 6, 7, 8]  # Q1-8\n",
    "NUM_TRIALS = 3  # Run each test multiple times\n",
    "\n",
    "print(f\"Test Configuration:\")\n",
    "print(f\"  Questions: {QUESTIONS}\")\n",
    "print(f\"  Trials per task: {NUM_TRIALS}\")\n",
    "print(f\"\\nTasks:\")\n",
    "for i, task in enumerate(TEST_TASKS, 1):\n",
    "    print(f\"  Task {i}: {task['name']} (focus: {task['primary_domain']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Runner Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "async def run_standard(query: str) -> dict:\n    \"\"\"Run standard agent and extract outputs.\"\"\"\n    try:\n        trajectory = []\n        final_response = \"\"\n        all_messages = []\n        \n        async for chunk in standard_agent.astream(\n            {\"messages\": [(\"user\", query)]},\n            stream_mode=\"updates\",\n        ):\n            if isinstance(chunk, dict):\n                for key in ['tools', 'model']:\n                    if key in chunk:\n                        msgs = chunk[key].get('messages', [])\n                        all_messages.extend(msgs)\n                        for msg in msgs:\n                            tool_calls = extract_tool_calls_from_message(msg)\n                            trajectory.extend(tool_calls)\n        \n        for msg in reversed(all_messages):\n            if hasattr(msg, 'content') and msg.content:\n                final_response = msg.content\n                break\n        \n        return {\"final_response\": final_response, \"trajectory\": trajectory, \"error\": None}\n    except Exception as e:\n        return {\"final_response\": \"\", \"trajectory\": [], \"error\": str(e)}\n\nasync def run_test(task_idx: int, questions: list, agent_type: str) -> dict:\n    \"\"\"Run a single test and return results.\"\"\"\n    task = TEST_TASKS[task_idx]\n    partial_task = build_partial_task(task, questions)\n    reference = build_reference_outputs(partial_task)\n    inputs = {\"query\": partial_task[\"query\"]}\n    \n    if agent_type == \"standard\":\n        outputs = await run_standard(inputs[\"query\"])\n    else:\n        outputs = await run_multi_agent(inputs[\"query\"])\n    \n    if outputs.get(\"error\"):\n        return {\"score\": 0.0, \"error\": outputs[\"error\"], \"correct\": 0, \"total\": len(questions)}\n    \n    result = recall_accuracy_evaluator(inputs, outputs, reference)\n    return {\n        \"score\": result[\"score\"],\n        \"error\": None,\n        \"correct\": int(result[\"score\"] * len(questions)),\n        \"total\": len(questions)\n    }\n\nprint(\"Runner functions defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run All Tests\n",
    "\n",
    "Running {NUM_TRIALS} trials for each task/agent combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run all tests\nall_results = []\n\nfor task_idx in range(len(TEST_TASKS)):\n    task = TEST_TASKS[task_idx]\n    print(f\"\\n{'='*60}\")\n    print(f\"Task {task_idx + 1}: {task['name']}\")\n    print(f\"{'='*60}\")\n    \n    for trial in range(NUM_TRIALS):\n        print(f\"\\n  Trial {trial + 1}/{NUM_TRIALS}\")\n        \n        # Run standard agent\n        print(f\"    Standard agent...\", end=\" \", flush=True)\n        std_result = await run_test(task_idx, QUESTIONS, \"standard\")\n        std_status = f\"{std_result['correct']}/{std_result['total']}\" if not std_result['error'] else \"ERROR\"\n        print(std_status)\n        \n        # Run multi-agent  \n        print(f\"    Multi-agent...\", end=\" \", flush=True)\n        multi_result = await run_test(task_idx, QUESTIONS, \"multi\")\n        multi_status = f\"{multi_result['correct']}/{multi_result['total']}\" if not multi_result['error'] else \"ERROR\"\n        print(multi_status)\n        \n        all_results.append({\n            'task': task_idx + 1,\n            'task_name': task['name'],\n            'trial': trial + 1,\n            'standard_score': std_result['score'],\n            'standard_correct': std_result['correct'],\n            'multi_score': multi_result['score'],\n            'multi_correct': multi_result['correct'],\n            'total': std_result['total']\n        })\n\nprint(f\"\\n{'='*60}\")\nprint(\"All tests completed!\")\nprint(f\"{'='*60}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create results dataframe\ndf = pd.DataFrame(all_results)\n\n# Summary by task\nprint(\"Results by Task (averaged across trials):\")\nprint(\"-\" * 50)\ntask_summary = df.groupby('task').agg({\n    'standard_score': ['mean', 'std'],\n    'multi_score': ['mean', 'std'],\n    'standard_correct': 'mean',\n    'multi_correct': 'mean',\n    'total': 'first'\n}).round(3)\n\nfor task_idx in range(1, len(TEST_TASKS) + 1):\n    task_data = df[df['task'] == task_idx]\n    std_mean = task_data['standard_score'].mean()\n    std_std = task_data['standard_score'].std()\n    multi_mean = task_data['multi_score'].mean()\n    multi_std = task_data['multi_score'].std()\n    \n    print(f\"\\nTask {task_idx}: {TEST_TASKS[task_idx-1]['name']}\")\n    print(f\"  Standard: {std_mean:.1%} +/- {std_std:.1%}\")\n    print(f\"  Multi:    {multi_mean:.1%} +/- {multi_std:.1%}\")\n    print(f\"  Winner:   {'Multi' if multi_mean > std_mean else 'Standard' if std_mean > multi_mean else 'Tie'}\")\n\n# Overall summary\nprint(\"\\n\" + \"=\" * 50)\nprint(\"OVERALL RESULTS\")\nprint(\"=\" * 50)\nstd_overall = df['standard_score'].mean()\nmulti_overall = df['multi_score'].mean()\nprint(f\"  Standard Agent: {std_overall:.1%}\")\nprint(f\"  Multi-Agent:    {multi_overall:.1%}\")\nprint(f\"  Improvement:    {(multi_overall - std_overall)*100:+.1f} percentage points\")\nprint(f\"\\n  Multi-agent wins: {(df['multi_score'] > df['standard_score']).sum()}/{len(df)} trials\")\nprint(f\"  Standard wins:    {(df['standard_score'] > df['multi_score']).sum()}/{len(df)} trials\")\nprint(f\"  Ties:             {(df['standard_score'] == df['multi_score']).sum()}/{len(df)} trials\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display full results table\ndisplay(Markdown(\"### All Trial Results\"))\ndisplay(df[['task', 'trial', 'standard_correct', 'multi_correct', 'total', 'standard_score', 'multi_score']]\n        .rename(columns={\n            'task': 'Task',\n            'trial': 'Trial', \n            'standard_correct': 'Standard Correct',\n            'multi_correct': 'Multi Correct',\n            'total': 'Total Questions',\n            'standard_score': 'Standard Score',\n            'multi_score': 'Multi Score'\n        }))\n\n# Visualization\nimport plotly.graph_objects as go\n\n# Calculate averages per task\ntask_avgs = df.groupby('task').agg({\n    'standard_score': 'mean',\n    'multi_score': 'mean'\n}).reset_index()\n\ntasks = [f\"Task {i}\" for i in task_avgs['task']]\nstd_scores = task_avgs['standard_score'].tolist()\nmulti_scores = task_avgs['multi_score'].tolist()\n\nfig = go.Figure()\n\nfig.add_trace(go.Bar(\n    name='Standard Agent',\n    x=tasks,\n    y=std_scores,\n    marker_color='#1f77b4',\n    text=[f\"{s:.1%}\" for s in std_scores],\n    textposition='outside'\n))\n\nfig.add_trace(go.Bar(\n    name='Multi-Agent',\n    x=tasks,\n    y=multi_scores,\n    marker_color='#2ca02c',\n    text=[f\"{s:.1%}\" for s in multi_scores],\n    textposition='outside'\n))\n\n# Add average lines\nstd_avg = df['standard_score'].mean()\nmulti_avg = df['multi_score'].mean()\n\nfig.add_trace(go.Scatter(\n    x=tasks,\n    y=[std_avg] * len(tasks),\n    mode='lines',\n    name=f'Standard Avg ({std_avg:.1%})',\n    line=dict(color='#1f77b4', width=2, dash='dash'),\n))\n\nfig.add_trace(go.Scatter(\n    x=tasks,\n    y=[multi_avg] * len(tasks),\n    mode='lines',\n    name=f'Multi-Agent Avg ({multi_avg:.1%})',\n    line=dict(color='#2ca02c', width=2, dash='dash'),\n))\n\nfig.update_layout(\n    title=\"Recall Accuracy: Standard Agent vs Multi-Agent\",\n    xaxis_title=\"Test Case\",\n    yaxis_title=\"Recall Accuracy\",\n    barmode='group',\n    height=450,\n    yaxis=dict(range=[0, 1.0], tickformat='.0%'),\n    showlegend=True,\n    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n)\n\nfig.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Comparing the Two Approaches\n\n| Aspect | Standard ReAct Agent | Multi-Agent (Supervisor/Worker) |\n|--------|---------------------|--------------------------------|\n| **Architecture** | Single agent processes all questions | Supervisor delegates to isolated workers |\n| **Context** | All tool calls accumulate in one history | Each worker has fresh, isolated context |\n| **Execution** | Sequential tool calls in shared state | Parallel workers with independent state |\n| **Pros** | Simple implementation, direct context access | Context isolation, focused workers, structured decomposition |\n| **Cons** | Context grows unbounded, values get \"lost\" | More complex, delegation overhead, supervisor accuracy matters |\n| **Best for** | Simple, single-domain tasks | Complex, multi-domain tasks with many data points |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n### Context Isolation Helps\n\nThe Multi-Agent's context isolation approach shows promise for reducing confusion in multi-domain tasks:\n- Workers process one sub-task at a time without accumulated context noise\n- Explicit delegation forces task decomposition\n- Results are explicitly passed back rather than implicitly remembered\n\n### When to Use Each Approach\n\n| Aspect | Standard Agent | Multi-Agent |\n|--------|----------------|-------------|\n| **Best for** | Simple, single-domain tasks | Complex, multi-domain tasks |\n| **Context size** | Grows unbounded | Isolated per worker |\n| **Complexity** | Low | Higher |\n| **Latency** | Lower | Higher (multiple agent calls) |\n\n### Next Steps\n\nBased on these results:\n1. **Tune delegation prompts** - Help supervisor make better task breakdowns\n2. **Add verification** - Have workers verify calculated values before reporting\n3. **Expand evaluation** - Test on more diverse task types"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}