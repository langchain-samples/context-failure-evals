{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Context Distraction: Standard Agent vs Deep Agent Comparison\n\nThis notebook compares standard ReAct agent performance against the Deep Agent (supervisor/worker delegation pattern) on context distraction tasks.\n\n## What We're Testing\n\nBoth agents answer 8 questions (Q1-8) requiring research across 5 technology domains. The Deep Agent uses a supervisor that delegates to workers with isolated context, while the Standard Agent processes everything in a single context.\n\n**Hypothesis**: Deep Agent's context isolation should improve recall accuracy on multi-step tasks."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imports and Setup\nimport asyncio\nfrom typing import Dict, Any, List\nimport pandas as pd\nimport numpy as np\nfrom IPython.display import display, Markdown\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\n# Import test infrastructure\nfrom context_distraction.resources.test_tasks import TEST_TASKS, build_partial_task\nfrom context_distraction.tests.evaluators import recall_accuracy_evaluator\nfrom context_distraction.tests.setup_datasets import build_reference_outputs\nfrom context_distraction.resources.validation_utils import extract_tool_calls_from_message\n\n# Import agents\nfrom context_distraction.agent import agent as standard_agent\nfrom context_distraction.deep import run_deep_agent\n\nprint(\"Setup complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Test Configuration\n\nRunning Q1-8 on all 3 tasks with multiple trials to measure consistency."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nQUESTIONS = [1, 2, 3, 4, 5, 6, 7, 8]  # Q1-8\nNUM_TRIALS = 3  # Run each test multiple times\n\nprint(f\"Test Configuration:\")\nprint(f\"  Questions: {QUESTIONS}\")\nprint(f\"  Trials per task: {NUM_TRIALS}\")\nprint(f\"\\nTasks:\")\nfor i, task in enumerate(TEST_TASKS, 1):\n    print(f\"  Task {i}: {task['name']} (focus: {task['primary_domain']})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Agent Runner Functions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "async def run_standard(query: str) -> dict:\n    \"\"\"Run standard agent and extract outputs.\"\"\"\n    try:\n        trajectory = []\n        final_response = \"\"\n        all_messages = []\n        \n        async for chunk in standard_agent.astream(\n            {\"messages\": [(\"user\", query)]},\n            stream_mode=\"updates\",\n        ):\n            if isinstance(chunk, dict):\n                for key in ['tools', 'model']:\n                    if key in chunk:\n                        msgs = chunk[key].get('messages', [])\n                        all_messages.extend(msgs)\n                        for msg in msgs:\n                            tool_calls = extract_tool_calls_from_message(msg)\n                            trajectory.extend(tool_calls)\n        \n        for msg in reversed(all_messages):\n            if hasattr(msg, 'content') and msg.content:\n                final_response = msg.content\n                break\n        \n        return {\"final_response\": final_response, \"trajectory\": trajectory, \"error\": None}\n    except Exception as e:\n        return {\"final_response\": \"\", \"trajectory\": [], \"error\": str(e)}\n\nasync def run_test(task_idx: int, questions: list, agent_type: str) -> dict:\n    \"\"\"Run a single test and return results.\"\"\"\n    task = TEST_TASKS[task_idx]\n    partial_task = build_partial_task(task, questions)\n    reference = build_reference_outputs(partial_task)\n    inputs = {\"query\": partial_task[\"query\"]}\n    \n    if agent_type == \"standard\":\n        outputs = await run_standard(inputs[\"query\"])\n    else:\n        outputs = await run_deep_agent(inputs[\"query\"])\n    \n    if outputs.get(\"error\"):\n        return {\"score\": 0.0, \"error\": outputs[\"error\"], \"correct\": 0, \"total\": len(questions)}\n    \n    result = recall_accuracy_evaluator(inputs, outputs, reference)\n    return {\n        \"score\": result[\"score\"],\n        \"error\": None,\n        \"correct\": int(result[\"score\"] * len(questions)),\n        \"total\": len(questions)\n    }\n\nprint(\"Runner functions defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Run All Tests\n\nRunning {NUM_TRIALS} trials for each task/agent combination."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run all tests\nall_results = []\n\nfor task_idx in range(len(TEST_TASKS)):\n    task = TEST_TASKS[task_idx]\n    print(f\"\\n{'='*60}\")\n    print(f\"Task {task_idx + 1}: {task['name']}\")\n    print(f\"{'='*60}\")\n    \n    for trial in range(NUM_TRIALS):\n        print(f\"\\n  Trial {trial + 1}/{NUM_TRIALS}\")\n        \n        # Run standard agent\n        print(f\"    Standard agent...\", end=\" \", flush=True)\n        std_result = await run_test(task_idx, QUESTIONS, \"standard\")\n        std_status = f\"{std_result['correct']}/{std_result['total']}\" if not std_result['error'] else \"ERROR\"\n        print(std_status)\n        \n        # Run deep agent  \n        print(f\"    Deep agent...\", end=\" \", flush=True)\n        deep_result = await run_test(task_idx, QUESTIONS, \"deep\")\n        deep_status = f\"{deep_result['correct']}/{deep_result['total']}\" if not deep_result['error'] else \"ERROR\"\n        print(deep_status)\n        \n        all_results.append({\n            'task': task_idx + 1,\n            'task_name': task['name'],\n            'trial': trial + 1,\n            'standard_score': std_result['score'],\n            'standard_correct': std_result['correct'],\n            'deep_score': deep_result['score'],\n            'deep_correct': deep_result['correct'],\n            'total': std_result['total']\n        })\n\nprint(f\"\\n{'='*60}\")\nprint(\"All tests completed!\")\nprint(f\"{'='*60}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Results Summary"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create results dataframe\ndf = pd.DataFrame(all_results)\n\n# Summary by task\nprint(\"Results by Task (averaged across trials):\")\nprint(\"-\" * 50)\ntask_summary = df.groupby('task').agg({\n    'standard_score': ['mean', 'std'],\n    'deep_score': ['mean', 'std'],\n    'standard_correct': 'mean',\n    'deep_correct': 'mean',\n    'total': 'first'\n}).round(3)\n\nfor task_idx in range(1, len(TEST_TASKS) + 1):\n    task_data = df[df['task'] == task_idx]\n    std_mean = task_data['standard_score'].mean()\n    std_std = task_data['standard_score'].std()\n    deep_mean = task_data['deep_score'].mean()\n    deep_std = task_data['deep_score'].std()\n    \n    print(f\"\\nTask {task_idx}: {TEST_TASKS[task_idx-1]['name']}\")\n    print(f\"  Standard: {std_mean:.1%} +/- {std_std:.1%}\")\n    print(f\"  Deep:     {deep_mean:.1%} +/- {deep_std:.1%}\")\n    print(f\"  Winner:   {'Deep' if deep_mean > std_mean else 'Standard' if std_mean > deep_mean else 'Tie'}\")\n\n# Overall summary\nprint(\"\\n\" + \"=\" * 50)\nprint(\"OVERALL RESULTS\")\nprint(\"=\" * 50)\nstd_overall = df['standard_score'].mean()\ndeep_overall = df['deep_score'].mean()\nprint(f\"  Standard Agent: {std_overall:.1%}\")\nprint(f\"  Deep Agent:     {deep_overall:.1%}\")\nprint(f\"  Improvement:    {(deep_overall - std_overall)*100:+.1f} percentage points\")\nprint(f\"\\n  Deep agent wins: {(df['deep_score'] > df['standard_score']).sum()}/{len(df)} trials\")\nprint(f\"  Standard wins:   {(df['standard_score'] > df['deep_score']).sum()}/{len(df)} trials\")\nprint(f\"  Ties:            {(df['standard_score'] == df['deep_score']).sum()}/{len(df)} trials\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Detailed Results Table"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display full results table\ndisplay(Markdown(\"### All Trial Results\"))\ndisplay(df[['task', 'trial', 'standard_correct', 'deep_correct', 'total', 'standard_score', 'deep_score']]\n        .rename(columns={\n            'task': 'Task',\n            'trial': 'Trial', \n            'standard_correct': 'Standard Correct',\n            'deep_correct': 'Deep Correct',\n            'total': 'Total Questions',\n            'standard_score': 'Standard Score',\n            'deep_score': 'Deep Score'\n        }))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Comparing the Two Approaches\n\n### Standard ReAct Agent\n\n**How it works:**\n- Single agent context processes all questions\n- All tool calls and results accumulate in one message history\n- Agent must track multiple data points across domains simultaneously\n\n**Pros:**\n- Simpler implementation\n- Direct access to all context\n\n**Cons:**\n- Context grows with each tool call\n- Important values can get \"lost\" in long conversation history\n- Prone to confusion when handling multiple domains\n\n### Deep Agent (Supervisor/Worker Pattern)\n\n**How it works:**\n1. **Supervisor** analyzes the task and delegates sub-tasks to workers\n2. **Workers** execute in isolated contexts, focusing on one domain at a time\n3. Workers report results back to supervisor\n4. Supervisor synthesizes all worker results into final answer\n\n**Pros:**\n- Context isolation prevents cross-contamination\n- Workers focus on single domain/question\n- Supervisor can coordinate complex multi-step reasoning\n\n**Cons:**\n- More complex architecture\n- Additional overhead from delegation\n- Relies on supervisor making correct delegations"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Key Takeaways\n\n### Context Isolation Helps\n\nThe Deep Agent's context isolation approach shows promise for reducing confusion in multi-domain tasks:\n- Workers process one sub-task at a time without accumulated context noise\n- Explicit delegation forces task decomposition\n- Results are explicitly passed back rather than implicitly remembered\n\n### When to Use Each Approach\n\n| Aspect | Standard Agent | Deep Agent |\n|--------|----------------|------------|\n| **Best for** | Simple, single-domain tasks | Complex, multi-domain tasks |\n| **Context size** | Grows unbounded | Isolated per worker |\n| **Complexity** | Low | Higher |\n| **Latency** | Lower | Higher (multiple agent calls) |\n\n### Next Steps\n\nBased on these results:\n1. **Tune delegation prompts** - Help supervisor make better task breakdowns\n2. **Add verification** - Have workers verify calculated values before reporting\n3. **Expand evaluation** - Test on more diverse task types"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}