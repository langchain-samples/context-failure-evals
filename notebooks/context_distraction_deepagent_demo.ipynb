{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Context Distraction: Comparing Custom Graph vs DeepAgents Framework\n\nThis notebook compares two approaches to context isolation for complex, multi-step research tasks.\n\n## The Problem\n\nAs LLM agents perform research tasks with many operations, each tool call and result accumulates in the conversation context. With complex tasks requiring dozens of tool calls, the context becomes extremely long. **LLMs struggle to maintain recall accuracy over very long contexts** - this is called **context distraction**.\n\n## What We'll Compare\n\nWe'll evaluate two context-isolation approaches on multi-domain investment research tasks:\n\n1. **Custom Graph Agent** - Our hand-crafted LangGraph implementation\n   - Uses supervisor/researcher pattern with explicit subgraphs\n   - Custom state management and deliverable tracking\n   - Manually designed workflow nodes\n\n2. **DeepAgents Framework** - LangChain's open-source agent harness\n   - Built-in `task` tool for spawning subagents\n   - Built-in filesystem for intermediate results\n   - Built-in planning with todos\n   - Automatic summarization\n\n**Goal**: Determine if the simpler DeepAgents approach can match or exceed the custom graph agent's recall accuracy."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import asyncio\n",
    "from typing import Dict, Any, List\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Import our test infrastructure\n",
    "from context_distraction.resources.test_tasks import TEST_TASKS\n",
    "from context_distraction.tests.evaluators import (\n",
    "    recall_accuracy_evaluator,\n",
    "    tool_call_completeness_evaluator,\n",
    "    tool_call_efficiency_evaluator,\n",
    "    extract_answers_json_from_text,\n",
    ")\n",
    "from context_distraction.tests.setup_datasets import build_reference_outputs\n",
    "from context_distraction.resources.validation_utils import extract_tool_calls_from_message\n",
    "\n",
    "print(\"\u2713 Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Research Tasks\n",
    "\n",
    "We'll evaluate agents on 3 complex investment analysis tasks covering 5 technology sectors each:\n",
    "- **Task 1**: Focus on Renewable Energy\n",
    "- **Task 2**: Focus on Electric Vehicles\n",
    "- **Task 3**: Focus on Biotechnology\n",
    "\n",
    "Each task requires gathering statistics, expert opinions, case studies, and performing financial calculations including compound growth projections, cost-benefit analysis with NPV, correlation analyses, and investment portfolio optimization.\n",
    "\n",
    "Each agent must answer 9 specific questions requiring precise recall of facts from throughout the research process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all test cases\n",
    "print(\"Test Cases:\")\n",
    "for i, task in enumerate(TEST_TASKS, 1):\n",
    "    print(f\"\\n{i}. {task['name']}\")\n",
    "    print(f\"   Primary domain: {task['primary_domain']}\")\n",
    "    print(f\"   All domains: {', '.join(task['topics'])}\")\n",
    "    print(f\"   Questions to answer: {len(task['recall_questions'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Agent Runner Functions\n\nThese functions run each agent type and extract structured outputs for evaluation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import agents\nfrom context_distraction.graph import graph as graph_agent\nfrom context_distraction.deepagent import deep_agent, run_deep_agent\nfrom langchain_core.messages import HumanMessage\n\nasync def run_graph_agent(query: str) -> dict:\n    \"\"\"Run graph agent with recursion_limit=200 and extract outputs.\"\"\"\n    try:\n        trajectory = []\n        final_response = \"\"\n        all_messages = []\n        \n        # Set recursion limit to 200 for complex tasks\n        config = {\"recursion_limit\": 200}\n        \n        async for chunk in graph_agent.astream(\n            {\"supervisor_messages\": [HumanMessage(content=query)]},\n            config=config,\n            subgraphs=True,\n            stream_mode=\"updates\",\n        ):\n            if isinstance(chunk, tuple) and len(chunk) >= 2:\n                namespace, data = chunk\n            elif isinstance(chunk, dict):\n                data = chunk\n            else:\n                continue\n            \n            if isinstance(data, dict):\n                for node_key, node_data in data.items():\n                    if isinstance(node_data, dict):\n                        for msg_key in ['supervisor_messages', 'reseacher_messages', 'messages']:\n                            if msg_key in node_data and isinstance(node_data[msg_key], list):\n                                msgs = node_data[msg_key]\n                                all_messages.extend(msgs)\n                                \n                                for msg in msgs:\n                                    tool_calls = extract_tool_calls_from_message(msg)\n                                    for tc in tool_calls:\n                                        trajectory.append(tc)\n        \n        # Extract final response\n        for msg in reversed(all_messages):\n            if isinstance(msg, dict) and msg.get(\"content\"):\n                final_response = msg[\"content\"]\n                break\n            elif hasattr(msg, 'content') and msg.content:\n                final_response = msg.content\n                break\n        \n        return {\"final_response\": final_response, \"trajectory\": trajectory, \"error\": None}\n    except Exception as e:\n        return {\"final_response\": \"\", \"trajectory\": [], \"error\": str(e)}\n\nprint(\"\u2713 Defined agent runners\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Run All Test Cases on Both Agents\n\nLet's run all 3 test cases on both agents and evaluate their performance.\n\n**Note:** This will take approximately 30-45 minutes as each test case runs sequentially (Graph agent, then DeepAgent)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run all test cases sequentially\nall_results = []\n\nfor i, task in enumerate(TEST_TASKS, 1):\n    print(f\"\\n{'='*80}\")\n    print(f\"Running Test Case {i}: {task['name']}\")\n    print(f\"{'='*80}\\n\")\n    \n    reference_outputs = build_reference_outputs(task)\n    inputs = {\"query\": task[\"query\"]}\n    \n    # Run graph agent (baseline)\n    print(f\"Running Graph agent...\", flush=True)\n    graph_outputs = await run_graph_agent(task[\"query\"])\n    if graph_outputs.get(\"error\"):\n        print(f\"  \u2717 FAILED: {graph_outputs['error']}\", flush=True)\n        graph_recall = {\"score\": 0.0}\n        graph_completeness = {\"score\": 0.0}\n        graph_efficiency = {\"score\": 0.0}\n    else:\n        print(f\"  \u2713 Completed {len(graph_outputs['trajectory'])} tool calls\", flush=True)\n        graph_recall = recall_accuracy_evaluator(inputs, graph_outputs, reference_outputs)\n        graph_completeness = tool_call_completeness_evaluator(inputs, graph_outputs, reference_outputs)\n        graph_efficiency = tool_call_efficiency_evaluator(inputs, graph_outputs, reference_outputs)\n    \n    # Run deep agent\n    print(f\"Running DeepAgent...\", flush=True)\n    deep_outputs = await run_deep_agent(task[\"query\"])\n    if deep_outputs.get(\"error\"):\n        print(f\"  \u2717 FAILED: {deep_outputs['error']}\", flush=True)\n        deep_recall = {\"score\": 0.0}\n        deep_completeness = {\"score\": 0.0}\n        deep_efficiency = {\"score\": 0.0}\n    else:\n        print(f\"  \u2713 Completed {len(deep_outputs['trajectory'])} tool calls\", flush=True)\n        deep_recall = recall_accuracy_evaluator(inputs, deep_outputs, reference_outputs)\n        deep_completeness = tool_call_completeness_evaluator(inputs, deep_outputs, reference_outputs)\n        deep_efficiency = tool_call_efficiency_evaluator(inputs, deep_outputs, reference_outputs)\n    \n    # Store results\n    all_results.append({\n        'case': i,\n        'name': task['name'],\n        'primary_domain': task['primary_domain'],\n        'graph': {\n            'recall': graph_recall['score'],\n            'completeness': graph_completeness['score'],\n            'efficiency': graph_efficiency['score'],\n            'tool_calls': len(graph_outputs['trajectory']),\n            'failed': bool(graph_outputs.get(\"error\"))\n        },\n        'deep': {\n            'recall': deep_recall['score'],\n            'completeness': deep_completeness['score'],\n            'efficiency': deep_efficiency['score'],\n            'tool_calls': len(deep_outputs['trajectory']),\n            'failed': bool(deep_outputs.get(\"error\"))\n        }\n    })\n    \n    print(f\"\\n  Graph:     {graph_recall['score']:.1%} recall, {graph_completeness['score']:.1%} completeness{'  [FAILED]' if graph_outputs.get('error') else ''}\")\n    print(f\"  DeepAgent: {deep_recall['score']:.1%} recall, {deep_completeness['score']:.1%} completeness{'  [FAILED]' if deep_outputs.get('error') else ''}\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"\u2713 All test cases completed\")\nprint(f\"{'='*80}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary: Individual Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create detailed results table\nresults_data = []\nfor result in all_results:\n    graph_recall = f\"{result['graph']['recall']:.1%}\"\n    deep_recall = f\"{result['deep']['recall']:.1%}\"\n    \n    if result['graph'].get('failed'):\n        graph_recall += \" (FAILED)\"\n    if result['deep'].get('failed'):\n        deep_recall += \" (FAILED)\"\n    \n    results_data.append({\n        'Test Case': f\"Case {result['case']}\",\n        'Domain': result['primary_domain'].replace('_', ' ').title(),\n        'Graph Recall': graph_recall,\n        'DeepAgent Recall': deep_recall,\n        'Difference': f\"{(result['deep']['recall'] - result['graph']['recall']) * 100:+.1f}pp\"\n    })\n\nresults_df = pd.DataFrame(results_data)\ndisplay(results_df)\n\nprint(\"\\n\ud83d\udcca Individual Case Analysis:\")\nfor result in all_results:\n    diff = (result['deep']['recall'] - result['graph']['recall']) * 100\n    graph_status = \" (FAILED)\" if result['graph'].get('failed') else \"\"\n    deep_status = \" (FAILED)\" if result['deep'].get('failed') else \"\"\n    print(f\"   Case {result['case']}: DeepAgent {result['deep']['recall']:.1%}{deep_status} vs Graph {result['graph']['recall']:.1%}{graph_status} ({diff:+.1f}pp)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Performance Comparison\n",
    "\n",
    "Let's calculate and visualize the average performance across all 3 test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate averages\navg_graph_recall = np.mean([r['graph']['recall'] for r in all_results])\navg_deep_recall = np.mean([r['deep']['recall'] for r in all_results])\navg_graph_completeness = np.mean([r['graph']['completeness'] for r in all_results])\navg_deep_completeness = np.mean([r['deep']['completeness'] for r in all_results])\navg_graph_efficiency = np.mean([r['graph']['efficiency'] for r in all_results])\navg_deep_efficiency = np.mean([r['deep']['efficiency'] for r in all_results])\n\n# Create average comparison chart\nfig = go.Figure()\n\nagents = [\"Graph Agent\\n(Custom LangGraph)\", \"DeepAgent\\n(Framework)\"]\n\nfig.add_trace(go.Bar(\n    name='Recall Accuracy',\n    x=agents,\n    y=[avg_graph_recall, avg_deep_recall],\n    marker_color='#1f77b4',\n    text=[f\"{avg_graph_recall:.1%}\", f\"{avg_deep_recall:.1%}\"],\n    textposition='outside'\n))\n\nfig.add_trace(go.Bar(\n    name='Tool Call Completeness',\n    x=agents,\n    y=[avg_graph_completeness, avg_deep_completeness],\n    marker_color='#2ca02c',\n    text=[f\"{avg_graph_completeness:.1%}\", f\"{avg_deep_completeness:.1%}\"],\n    textposition='outside'\n))\n\nfig.add_trace(go.Bar(\n    name='Tool Call Efficiency',\n    x=agents,\n    y=[avg_graph_efficiency, avg_deep_efficiency],\n    marker_color='#ff7f0e',\n    text=[f\"{avg_graph_efficiency:.2f}\", f\"{avg_deep_efficiency:.2f}\"],\n    textposition='outside'\n))\n\nfig.update_layout(\n    title=\"Average Performance: Graph Agent vs DeepAgent\",\n    yaxis_title=\"Score\",\n    barmode='group',\n    height=500,\n    yaxis=dict(range=[0, 1.1]),\n    showlegend=True\n)\n\nfig.show()\n\n# Print summary statistics\nprint(\"\\n\ud83d\udcca AVERAGE RESULTS (across all 3 test cases):\")\nprint(f\"\\n  Graph Agent (Custom LangGraph):\")\nprint(f\"    - Recall Accuracy: {avg_graph_recall:.1%}\")\nprint(f\"    - Tool Completeness: {avg_graph_completeness:.1%}\")\nprint(f\"    - Tool Efficiency: {avg_graph_efficiency:.2f}\")\nprint(f\"\\n  DeepAgent (Framework):\")\nprint(f\"    - Recall Accuracy: {avg_deep_recall:.1%}\")\nprint(f\"    - Tool Completeness: {avg_deep_completeness:.1%}\")\nprint(f\"    - Tool Efficiency: {avg_deep_efficiency:.2f}\")\nprint(f\"\\n  \ud83d\udcc8 Difference (DeepAgent - Graph):\")\nrecall_diff = (avg_deep_recall - avg_graph_recall) * 100\nprint(f\"    - Recall Accuracy: {recall_diff:+.1f} percentage points\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Case-by-Case Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create case-by-case comparison chart\ncases = [f\"Case {r['case']}\" for r in all_results]\ngraph_scores = [r['graph']['recall'] for r in all_results]\ndeep_scores = [r['deep']['recall'] for r in all_results]\n\nfig = go.Figure()\n\nfig.add_trace(go.Bar(\n    name='Graph Agent',\n    x=cases,\n    y=graph_scores,\n    marker_color='#1f77b4',\n    text=[f\"{s:.1%}\" for s in graph_scores],\n    textposition='outside'\n))\n\nfig.add_trace(go.Bar(\n    name='DeepAgent',\n    x=cases,\n    y=deep_scores,\n    marker_color='#2ca02c',\n    text=[f\"{s:.1%}\" for s in deep_scores],\n    textposition='outside'\n))\n\n# Add average lines\nfig.add_trace(go.Scatter(\n    x=cases,\n    y=[avg_graph_recall] * len(cases),\n    mode='lines',\n    name='Graph Avg',\n    line=dict(color='#1f77b4', width=2, dash='dash'),\n    showlegend=True\n))\n\nfig.add_trace(go.Scatter(\n    x=cases,\n    y=[avg_deep_recall] * len(cases),\n    mode='lines',\n    name='DeepAgent Avg',\n    line=dict(color='#2ca02c', width=2, dash='dash'),\n    showlegend=True\n))\n\nfig.update_layout(\n    title=\"Recall Accuracy by Test Case\",\n    xaxis_title=\"Test Case\",\n    yaxis_title=\"Recall Accuracy\",\n    barmode='group',\n    height=500,\n    yaxis=dict(range=[0, 1.0]),\n    showlegend=True\n)\n\nfig.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Comparing the Two Approaches\n\n### Custom Graph Agent (LangGraph)\n\n**How it works:**\n1. **Planner node** extracts deliverables as structured output\n2. **Supervisor node** coordinates research via `deep_research` tool\n3. **Researcher subgraph** executes in isolated context, stores results via `store_deliverable`\n4. **Final report node** synthesizes all deliverables\n\n**Pros:**\n- Explicit workflow control\n- Guaranteed execution order (plan \u2192 research \u2192 report)\n- Custom state management for deliverables\n\n**Cons:**\n- Complex implementation (~250 lines of graph code)\n- Manual state passing between nodes\n- Requires understanding LangGraph internals\n\n### DeepAgent Framework\n\n**How it works:**\n1. Built-in `task` tool spawns subagents with isolated context\n2. Built-in filesystem for storing/sharing intermediate results\n3. Built-in todo tracking for planning\n4. Automatic summarization when context grows\n\n**Pros:**\n- Simpler implementation (~150 lines including prompts)\n- Framework handles state propagation\n- Built-in context management features\n- Easier to extend and customize\n\n**Cons:**\n- Less explicit workflow control (relies on prompting)\n- May require prompt tuning for specific use cases\n- Filesystem operations add some overhead"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n### Context Isolation Works\n\nBoth approaches demonstrate that **context isolation is essential** for complex, multi-step tasks:\n- Spawning subagents/researchers prevents context accumulation\n- Explicit storage of findings (state or filesystem) preserves critical information\n- Planning before execution helps organize complex workflows\n\n### Framework vs Custom Trade-offs\n\n| Aspect | Graph Agent | DeepAgent |\n|--------|-------------|-----------|\n| **Implementation complexity** | High | Low |\n| **Workflow control** | Explicit nodes | Prompt-driven |\n| **State management** | Custom | Built-in |\n| **Context handling** | Manual subgraph | Auto summarization |\n| **Extensibility** | Requires graph changes | Add tools/prompts |\n\n### Recommendations\n\n**Use DeepAgent framework when:**\n- You want simpler, faster implementation\n- The workflow can be guided by prompts\n- You need built-in features (filesystem, todos, summarization)\n- You're building general-purpose agents\n\n**Use custom LangGraph when:**\n- You need strict workflow control\n- You have complex state requirements\n- You need fine-grained observability\n- You're optimizing for a specific use case\n\n### Next Steps\n\nBased on these results, consider:\n1. **Prompt tuning** - Adjust supervisor/researcher prompts to improve recall\n2. **Hybrid approach** - Use DeepAgent with custom middleware for specific behaviors\n3. **Evaluation expansion** - Test on additional task types and domains"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}